{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0928 11:59:59.740470 10536 deprecation.py:323] From <ipython-input-3-cee6bf4d5582>:12: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0928 11:59:59.801374 10536 deprecation.py:506] From C:\\Users\\KITCOOP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0928 11:59:59.812278 10536 deprecation.py:506] From C:\\Users\\KITCOOP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력데이터의 shape (2, 3, 1)\n",
      "rnn이 출력하는 outputs의 의미 (2, 3, 100)\n",
      "(2, 100)\n",
      "(2, 100)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "values231 = np.array([\n",
    "    [[1],[2],[3]],\n",
    "    [[2],[3],[4]]\n",
    "])\n",
    "print(\"입력데이터의 shape\",values231.shape)\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "tf_values231 = tf.constant(values231,dtype=tf.float32) #상수화\n",
    "lstm_cell = tf.contrib.rnn.LSTMCell(num_units=100) #Long short term memory : hidden 100\n",
    "# static_rnn (사이즈 고정), LSTMCell (사이즈 유동)\n",
    "outputs,state = tf.nn.dynamic_rnn(cell=lstm_cell,dtype=tf.float32,inputs=tf_values231)\n",
    "print(\"rnn이 출력하는 outputs의 의미\",outputs.shape)\n",
    "print(state.c.shape)\n",
    "print(state.h.shape)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    output_run,state_run = sess.run([outputs,state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0928 12:01:36.794296   800 deprecation.py:323] From <ipython-input-5-67c7e68559c2>:14: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W0928 12:01:36.794296   800 deprecation.py:323] From <ipython-input-5-67c7e68559c2>:15: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처음 데이터 특성: [[ 0.30741334 -0.32884315 -0.6542847  -0.9385059   0.52089024]\n",
      " [ 0.99122757 -0.9542541  -0.7518079  -0.9995208   0.9820235 ]\n",
      " [ 0.9999268  -0.99783254 -0.8247353  -0.9999963   0.99947774]\n",
      " [ 0.996771   -0.68750614  0.8419969   0.9303911   0.8120684 ]] 차수: (4, 5)\n",
      "둘째 데이터 특성: [[ 0.99998885 -0.99976057 -0.06679279 -0.9999803   0.99982214]\n",
      " [-0.65249425 -0.51520866 -0.37968954 -0.5922594  -0.08968391]\n",
      " [ 0.998624   -0.99715203 -0.03308626 -0.9991566   0.9932902 ]\n",
      " [ 0.99681675 -0.9598194   0.39660627 -0.8307606   0.79671973]] 차수: (4, 5)\n"
     ]
    }
   ],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "n_inputs = 3\n",
    "n_neurons = 5\n",
    "reset_graph()\n",
    "X0 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "X1 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "output_seqs, states = tf.contrib.rnn.static_rnn(basic_cell, [X0,X1], dtype=tf.float32)\n",
    "Y0,Y1 = output_seqs\n",
    "init = tf.global_variables_initializer()\n",
    "X0_batch = np.array([[0,1,2],[3,4,5],[6,7,8],[9,0,1]])\n",
    "X1_batch = np.array([[9,8,7],[0,0,0],[6,5,4],[3,2,1]])\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    Y0_val,Y1_val = sess.run([Y0,Y1],feed_dict={X0:X0_batch,X1:X1_batch})\n",
    "print(\"처음 데이터 특성:\",Y0_val,\"차수:\",Y0_val.shape)\n",
    "print(\"둘째 데이터 특성:\",Y1_val,\"차수:\",Y1_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0928 12:01:23.199348   800 deprecation.py:323] From <ipython-input-3-4897f7b64e2c>:18: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0928 12:01:23.255816   800 deprecation.py:506] From C:\\Users\\KITCOOP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0928 12:01:23.268425   800 deprecation.py:506] From C:\\Users\\KITCOOP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"rnn/transpose_1:0\", shape=(3, 5, 100), dtype=float32)\n",
      "Tensor(\"rnn/while/Exit_3:0\", shape=(3, 100), dtype=float32)\n",
      "Tensor(\"rnn/while/Exit_4:0\", shape=(3, 100), dtype=float32)\n",
      "[[[ 2.75918487e-02 -1.20499395e-02  9.71389469e-03 ... -1.26772463e-01\n",
      "    9.42265689e-02  5.27448989e-02]\n",
      "  [ 5.06438166e-02 -3.68718468e-02 -1.85274606e-04 ... -2.50818908e-01\n",
      "    1.79454893e-01  1.10676132e-01]\n",
      "  [ 7.07166567e-02 -6.37025014e-02 -2.02977527e-02 ... -3.64470422e-01\n",
      "    2.56275982e-01  1.64060548e-01]\n",
      "  [ 8.91839266e-02 -8.64033401e-02 -4.58404422e-02 ... -4.62020755e-01\n",
      "    3.27252507e-01  2.08200753e-01]\n",
      "  [ 1.06444858e-01 -1.02855928e-01 -7.50569031e-02 ... -5.42413831e-01\n",
      "    3.94517988e-01  2.41960078e-01]]\n",
      "\n",
      " [[ 3.18476744e-02 -2.67104544e-02 -1.40933795e-02 ... -1.66149303e-01\n",
      "    1.16703235e-01  6.32562786e-02]\n",
      "  [ 5.57500944e-02 -5.73519096e-02 -3.64274532e-02 ... -3.09751272e-01\n",
      "    2.14303374e-01  1.27828062e-01]\n",
      "  [ 7.60036930e-02 -8.35622177e-02 -6.11990094e-02 ... -4.27264720e-01\n",
      "    2.98256457e-01  1.83328331e-01]\n",
      "  [ 9.43718255e-02 -1.02333844e-01 -8.74310657e-02 ... -5.20229936e-01\n",
      "    3.74028504e-01  2.26138383e-01]\n",
      "  [ 1.11009523e-01 -1.14007451e-01 -1.15652822e-01 ... -5.93253076e-01\n",
      "    4.44076329e-01  2.56539315e-01]]\n",
      "\n",
      " [[ 3.43517885e-02 -3.87982354e-02 -3.91184837e-02 ... -1.98728159e-01\n",
      "    1.41146228e-01  7.36704245e-02]\n",
      "  [ 5.86728379e-02 -7.29228705e-02 -7.21526518e-02 ... -3.56740236e-01\n",
      "    2.51596361e-01  1.43477008e-01]\n",
      "  [ 7.89532214e-02 -9.73915458e-02 -9.97965783e-02 ... -4.76182789e-01\n",
      "    3.42980355e-01  1.99529082e-01]\n",
      "  [ 9.69279781e-02 -1.12379029e-01 -1.25586167e-01 ... -5.65435231e-01\n",
      "    4.23057616e-01  2.39874914e-01]\n",
      "  [ 1.12632982e-01 -1.20121762e-01 -1.51997909e-01 ... -6.33172810e-01\n",
      "    4.94271576e-01  2.66366035e-01]]]\n",
      "LSTMStateTuple(c=array([[ 1.94927916e-01, -2.63798773e-01, -1.52552351e-01,\n",
      "        -5.56334376e-01, -1.26917589e+00, -4.72724557e-01,\n",
      "        -6.01098776e-01, -3.81854057e-01, -6.18447065e-01,\n",
      "         3.88637692e-01, -4.11294103e-01,  5.39977252e-02,\n",
      "         8.44934702e-01, -6.86202824e-01, -1.34521797e-01,\n",
      "        -1.19701314e+00, -5.20937920e-01, -4.04523790e-01,\n",
      "        -2.44520396e-01, -1.17988288e-01, -1.39445603e-01,\n",
      "        -2.36111104e-01,  2.40198076e-01,  4.23109770e-01,\n",
      "        -4.06133682e-01, -1.04392314e+00, -1.17291796e+00,\n",
      "        -1.12324309e+00,  7.04602361e-01,  4.18279588e-01,\n",
      "         1.88551515e-01,  5.16940236e-01, -9.17176843e-01,\n",
      "        -2.78296173e-01,  1.52875945e-01, -8.53121877e-02,\n",
      "         8.85428965e-01, -6.77438498e-01, -1.26489758e+00,\n",
      "        -6.09042168e-01,  4.67540860e-01, -4.83705133e-01,\n",
      "         1.56321764e-01,  2.42461473e-01,  7.34192193e-01,\n",
      "        -3.36068198e-02, -1.27130389e+00, -8.05525243e-01,\n",
      "        -7.15808749e-01,  2.95870692e-01,  2.46344462e-01,\n",
      "        -1.92327090e-02, -8.29025805e-01, -1.23843861e+00,\n",
      "         2.87977099e-01,  4.61373806e-01, -6.86590075e-01,\n",
      "        -1.74716841e-02,  8.98161083e-02, -3.18232358e-01,\n",
      "        -6.67551458e-01, -2.25319862e-01,  4.79599506e-01,\n",
      "         5.33026278e-01, -3.09234917e-01, -4.48957384e-01,\n",
      "        -5.21783590e-01,  7.70747900e-01,  1.11250246e+00,\n",
      "        -6.98225379e-01,  2.93651849e-01, -2.76264459e-01,\n",
      "         1.44862962e+00, -6.96955472e-02,  8.06339234e-02,\n",
      "        -3.57158333e-01,  9.66252804e-01,  1.70449093e-01,\n",
      "        -8.09493482e-01, -2.23010778e-01,  1.01774013e+00,\n",
      "         4.92772311e-01, -1.08760333e+00, -4.48210537e-01,\n",
      "         7.07944095e-01, -3.05930734e-01,  2.04327315e-01,\n",
      "        -1.57239348e-01, -5.50400093e-02, -7.47632742e-01,\n",
      "         7.42378905e-02, -6.64095104e-01,  1.18288434e+00,\n",
      "         3.05163205e-01, -5.62245131e-01,  2.72098899e-01,\n",
      "        -1.10788822e-01, -1.01987088e+00,  7.69743323e-01,\n",
      "         3.42265785e-01],\n",
      "       [ 2.03515366e-01, -3.10126126e-01, -2.41899014e-01,\n",
      "        -5.65591335e-01, -1.45666397e+00, -5.49047351e-01,\n",
      "        -6.16348088e-01, -4.44728732e-01, -6.50463223e-01,\n",
      "         3.54347050e-01, -4.60501373e-01,  2.24946663e-02,\n",
      "         8.74710083e-01, -7.34266758e-01, -1.19099349e-01,\n",
      "        -1.37606502e+00, -5.49724221e-01, -4.60042238e-01,\n",
      "        -2.44214594e-01, -1.19075090e-01, -1.11004420e-01,\n",
      "        -2.83067793e-01,  2.09269583e-01,  4.88952547e-01,\n",
      "        -4.23134804e-01, -1.17132521e+00, -1.38940179e+00,\n",
      "        -1.23289013e+00,  7.03904688e-01,  4.55973506e-01,\n",
      "         1.93479627e-01,  6.22458935e-01, -1.10470068e+00,\n",
      "        -2.62326986e-01,  1.57088891e-01, -1.16904147e-01,\n",
      "         1.01261878e+00, -7.12336183e-01, -1.45923579e+00,\n",
      "        -7.00448751e-01,  4.86038387e-01, -5.10244846e-01,\n",
      "         1.22401193e-01,  2.78358579e-01,  7.93011725e-01,\n",
      "        -1.40415579e-02, -1.39822745e+00, -9.80170488e-01,\n",
      "        -8.19283307e-01,  3.07432324e-01,  3.14340979e-01,\n",
      "         4.18430381e-03, -9.40004587e-01, -1.42569137e+00,\n",
      "         3.87341946e-01,  4.70693231e-01, -7.76776195e-01,\n",
      "        -3.54147777e-02,  8.75788927e-02, -2.93831050e-01,\n",
      "        -7.66435385e-01, -3.08858961e-01,  5.02731204e-01,\n",
      "         6.37234926e-01, -3.22104692e-01, -5.64243019e-01,\n",
      "        -5.86685777e-01,  8.61170173e-01,  1.25912273e+00,\n",
      "        -8.26905072e-01,  2.75538981e-01, -2.47645497e-01,\n",
      "         1.78406787e+00, -8.45861807e-02,  1.13704756e-01,\n",
      "        -4.03638810e-01,  1.20661891e+00,  1.09256431e-01,\n",
      "        -1.00569844e+00, -2.31584877e-01,  1.13461554e+00,\n",
      "         6.39037371e-01, -1.24411988e+00, -4.79659557e-01,\n",
      "         8.30398321e-01, -3.10850024e-01,  1.90854818e-01,\n",
      "        -1.72051519e-01, -9.23793092e-02, -9.22313571e-01,\n",
      "         7.34335035e-02, -7.65313625e-01,  1.36664450e+00,\n",
      "         2.99376130e-01, -6.66732967e-01,  2.88397789e-01,\n",
      "        -9.88556445e-02, -1.11114025e+00,  8.82412076e-01,\n",
      "         3.50549996e-01],\n",
      "       [ 2.06713110e-01, -3.46846700e-01, -3.28054726e-01,\n",
      "        -5.62533617e-01, -1.62052751e+00, -6.15304053e-01,\n",
      "        -6.22752070e-01, -4.95482177e-01, -6.72800779e-01,\n",
      "         3.10504019e-01, -4.96439874e-01,  1.86177343e-03,\n",
      "         8.76665354e-01, -7.68541157e-01, -1.11110851e-01,\n",
      "        -1.53628707e+00, -5.85853338e-01, -5.02373338e-01,\n",
      "        -2.38827094e-01, -1.21922269e-01, -8.19414482e-02,\n",
      "        -3.20270747e-01,  1.66528642e-01,  5.45573235e-01,\n",
      "        -4.47777987e-01, -1.26834726e+00, -1.60993528e+00,\n",
      "        -1.30568016e+00,  6.74041629e-01,  4.87702787e-01,\n",
      "         1.94121510e-01,  7.21237183e-01, -1.28092313e+00,\n",
      "        -2.46372133e-01,  1.59848005e-01, -1.42655298e-01,\n",
      "         1.12711668e+00, -7.24925280e-01, -1.64281178e+00,\n",
      "        -7.71154583e-01,  5.02904892e-01, -5.35724699e-01,\n",
      "         9.15184841e-02,  2.97863543e-01,  8.44931662e-01,\n",
      "        -3.51448357e-03, -1.48612952e+00, -1.14955127e+00,\n",
      "        -9.03244615e-01,  3.20146024e-01,  3.73197913e-01,\n",
      "         2.27711685e-02, -1.02247190e+00, -1.58748221e+00,\n",
      "         4.87076759e-01,  4.77017015e-01, -8.60468984e-01,\n",
      "        -4.77831140e-02,  8.90731514e-02, -2.71980107e-01,\n",
      "        -8.50095153e-01, -3.89962226e-01,  5.04011869e-01,\n",
      "         7.25148916e-01, -3.22100997e-01, -6.68977678e-01,\n",
      "        -6.45554543e-01,  9.41772759e-01,  1.38422620e+00,\n",
      "        -9.42875028e-01,  2.67859042e-01, -2.16226488e-01,\n",
      "         2.09704399e+00, -9.51640159e-02,  1.55704647e-01,\n",
      "        -4.43507046e-01,  1.43390584e+00,  5.76495454e-02,\n",
      "        -1.20778906e+00, -2.34410375e-01,  1.22850347e+00,\n",
      "         7.97365844e-01, -1.41517568e+00, -5.06675065e-01,\n",
      "         9.45933223e-01, -3.11045885e-01,  1.77593067e-01,\n",
      "        -1.87223509e-01, -1.24759331e-01, -1.08857107e+00,\n",
      "         7.00285286e-02, -8.64543438e-01,  1.52184713e+00,\n",
      "         2.92708486e-01, -7.60035694e-01,  2.95198858e-01,\n",
      "        -9.06276330e-02, -1.16947162e+00,  1.00857162e+00,\n",
      "         3.52404714e-01]], dtype=float32), h=array([[ 0.10644486, -0.10285593, -0.0750569 , -0.2562139 , -0.3129551 ,\n",
      "        -0.2565533 , -0.28935182, -0.2437095 , -0.36175537,  0.233986  ,\n",
      "        -0.16330439,  0.03217698,  0.41262007, -0.3817905 , -0.0663699 ,\n",
      "        -0.35441768, -0.17620833, -0.18923488, -0.15754564, -0.03372383,\n",
      "        -0.0423616 , -0.1121752 ,  0.15750618,  0.118459  , -0.2203621 ,\n",
      "        -0.5507123 , -0.42522803, -0.35686427,  0.27037594,  0.22229686,\n",
      "         0.1154588 ,  0.1340008 , -0.46555704, -0.13894944,  0.08315865,\n",
      "        -0.04715179,  0.22809896, -0.31688258, -0.44919693, -0.4021836 ,\n",
      "         0.2553942 , -0.17637177,  0.09187979,  0.12806246,  0.18810417,\n",
      "        -0.0104594 , -0.3346277 , -0.22678885, -0.15335125,  0.15217778,\n",
      "         0.14142   , -0.01010456, -0.36970252, -0.45991343,  0.16194946,\n",
      "         0.2533531 , -0.28439072, -0.01266844,  0.03440446, -0.10683307,\n",
      "        -0.27716276, -0.14141434,  0.24677469,  0.19654398, -0.10565236,\n",
      "        -0.2448449 , -0.27048135,  0.24629968,  0.32893398, -0.23434387,\n",
      "         0.16210791, -0.11712026,  0.6202783 , -0.03117823,  0.03747807,\n",
      "        -0.16329005,  0.40259758,  0.08282528, -0.23510717, -0.09188242,\n",
      "         0.55449986,  0.3188185 , -0.5041323 , -0.12667021,  0.20796727,\n",
      "        -0.10346333,  0.09906608, -0.10714538, -0.02160827, -0.34936255,\n",
      "         0.04456402, -0.2175769 ,  0.35640088,  0.13335672, -0.17200525,\n",
      "         0.08193515, -0.04812266, -0.54241383,  0.394518  ,  0.24196008],\n",
      "       [ 0.11100952, -0.11400745, -0.11565282, -0.2614296 , -0.31175208,\n",
      "        -0.29471922, -0.29648948, -0.28746822, -0.39194798,  0.21987572,\n",
      "        -0.17667024,  0.01366497,  0.42390057, -0.41594753, -0.05992376,\n",
      "        -0.3681958 , -0.176343  , -0.20598318, -0.16297308, -0.03078241,\n",
      "        -0.03131268, -0.13482061,  0.14231187,  0.1212591 , -0.23036987,\n",
      "        -0.6089812 , -0.45421693, -0.3638426 ,  0.26556852,  0.24684949,\n",
      "         0.12121588,  0.14219904, -0.528707  , -0.13207537,  0.08628546,\n",
      "        -0.06585925,  0.22719054, -0.32421467, -0.47110254, -0.46196136,\n",
      "         0.26998228, -0.17999093,  0.0742628 ,  0.14665699,  0.18013512,\n",
      "        -0.00400605, -0.33490103, -0.2425192 , -0.14848499,  0.15802144,\n",
      "         0.17928997,  0.00217908, -0.40768266, -0.48285362,  0.21829534,\n",
      "         0.26261464, -0.31436938, -0.02654202,  0.03294086, -0.09068593,\n",
      "        -0.30327892, -0.19684452,  0.25789833,  0.2190128 , -0.10389567,\n",
      "        -0.30208814, -0.30085692,  0.24918099,  0.33665952, -0.26130018,\n",
      "         0.1551464 , -0.10231941,  0.6798027 , -0.03721656,  0.05120455,\n",
      "        -0.18568112,  0.45372522,  0.05447884, -0.25472194, -0.09376431,\n",
      "         0.61070025,  0.4098419 , -0.5566611 , -0.12344783,  0.21491629,\n",
      "        -0.10030226,  0.09294618, -0.12120087, -0.03434422, -0.40434957,\n",
      "         0.04419564, -0.22598784,  0.37066928,  0.12838498, -0.18663187,\n",
      "         0.07955727, -0.04303809, -0.5932531 ,  0.44407633,  0.25653931],\n",
      "       [ 0.11263298, -0.12012176, -0.15199791, -0.2620016 , -0.30417624,\n",
      "        -0.32670295, -0.30005443, -0.3241634 , -0.4181989 ,  0.19877237,\n",
      "        -0.18495952,  0.00114715,  0.42632958, -0.44397527, -0.05685143,\n",
      "        -0.3741634 , -0.17836674, -0.21547975, -0.16459735, -0.02837503,\n",
      "        -0.02138471, -0.15280667,  0.11712668,  0.11917898, -0.24423891,\n",
      "        -0.65476656, -0.4740181 , -0.3631087 ,  0.25272244,  0.26888338,\n",
      "         0.12438252,  0.14464238, -0.5779324 , -0.12573132,  0.08883514,\n",
      "        -0.08178707,  0.22228819, -0.3236    , -0.48383826, -0.509156  ,\n",
      "         0.28377983, -0.18308532,  0.05715561,  0.15715662,  0.1693815 ,\n",
      "        -0.0009157 , -0.32989815, -0.25018662, -0.13930151,  0.16408901,\n",
      "         0.2108023 ,  0.01172839, -0.4377617 , -0.49668863,  0.27298543,\n",
      "         0.2698916 , -0.33921653, -0.03695536,  0.03296161, -0.07702378,\n",
      "        -0.32225925, -0.25134918,  0.2604941 ,  0.23161514, -0.09830704,\n",
      "        -0.35072163, -0.32758322,  0.24540633,  0.33723846, -0.281674  ,\n",
      "         0.15347691, -0.08725149,  0.7219251 , -0.04129168,  0.06793268,\n",
      "        -0.20629689,  0.48898688,  0.02935771, -0.26333857, -0.09316389,\n",
      "         0.6574965 ,  0.498009  , -0.6067124 , -0.11853633,  0.21503608,\n",
      "        -0.09612394,  0.08675238, -0.13602848, -0.04405811, -0.44817182,\n",
      "         0.04229061, -0.22865179,  0.37638703,  0.12249806, -0.19464627,\n",
      "         0.07477675, -0.03947438, -0.6331728 ,  0.49427158,  0.26636603]],\n",
      "      dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "values231 = np.array([\n",
    "    [[1],[2],[3]],\n",
    "    [[2],[3],[4]]\n",
    "])\n",
    "\n",
    "values352 = np.array([\n",
    "    [[1,4],[2,5],[3,6],[4,7],[5,8]],\n",
    "    [[2,5],[3,6],[4,7],[5,8],[6,9]],\n",
    "    [[3,6],[4,7],[5,8],[6,9],[7,10]]\n",
    "])\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "tf.values231 = tf.constant(values352,dtype=tf.float32)\n",
    "lstm_cell = tf.contrib.rnn.LSTMCell(num_units=100)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=lstm_cell,dtype=tf.float32,inputs=tf.values231)\n",
    "print(outputs)\n",
    "print(state.c)\n",
    "print(state.h)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    output_run,state_run=sess.run([outputs,state])\n",
    "print(output_run)\n",
    "print(state_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0928 12:01:39.778441   800 deprecation.py:323] From <ipython-input-6-ab5d578c84b9>:11: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0928 12:01:40.117942   800 deprecation.py:323] From <ipython-input-6-ab5d578c84b9>:22: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0928 12:01:40.117942   800 deprecation.py:323] From C:\\Users\\KITCOOP\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0928 12:01:40.117942   800 deprecation.py:323] From C:\\Users\\KITCOOP\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0928 12:01:40.364125   800 deprecation.py:323] From C:\\Users\\KITCOOP\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0928 12:01:40.448262   800 deprecation.py:323] From C:\\Users\\KITCOOP\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "0 Train accuracy: 0.93333334 Test accuracy: 0.9311\n",
      "1 Train accuracy: 0.96666664 Test accuracy: 0.9522\n",
      "2 Train accuracy: 0.97333336 Test accuracy: 0.9582\n",
      "3 Train accuracy: 0.97333336 Test accuracy: 0.9641\n",
      "4 Train accuracy: 0.96 Test accuracy: 0.9657\n",
      "5 Train accuracy: 0.9866667 Test accuracy: 0.9722\n",
      "6 Train accuracy: 0.97333336 Test accuracy: 0.9688\n",
      "7 Train accuracy: 0.98 Test accuracy: 0.9715\n",
      "8 Train accuracy: 0.93333334 Test accuracy: 0.97\n",
      "9 Train accuracy: 0.9866667 Test accuracy: 0.9642\n",
      "10 Train accuracy: 0.99333334 Test accuracy: 0.9681\n",
      "11 Train accuracy: 0.96666664 Test accuracy: 0.9706\n",
      "12 Train accuracy: 0.9866667 Test accuracy: 0.9642\n",
      "13 Train accuracy: 0.9866667 Test accuracy: 0.9721\n",
      "14 Train accuracy: 0.97333336 Test accuracy: 0.9761\n",
      "15 Train accuracy: 1.0 Test accuracy: 0.9677\n",
      "16 Train accuracy: 0.98 Test accuracy: 0.9769\n",
      "17 Train accuracy: 0.98 Test accuracy: 0.9734\n",
      "18 Train accuracy: 0.9866667 Test accuracy: 0.9796\n",
      "19 Train accuracy: 0.9866667 Test accuracy: 0.9664\n",
      "20 Train accuracy: 0.98 Test accuracy: 0.9767\n",
      "21 Train accuracy: 0.98 Test accuracy: 0.9741\n",
      "22 Train accuracy: 0.98 Test accuracy: 0.9789\n",
      "23 Train accuracy: 1.0 Test accuracy: 0.9754\n",
      "24 Train accuracy: 1.0 Test accuracy: 0.9751\n",
      "25 Train accuracy: 1.0 Test accuracy: 0.978\n",
      "26 Train accuracy: 0.98 Test accuracy: 0.9769\n",
      "27 Train accuracy: 1.0 Test accuracy: 0.9763\n",
      "28 Train accuracy: 0.98 Test accuracy: 0.9785\n",
      "29 Train accuracy: 0.99333334 Test accuracy: 0.9785\n",
      "30 Train accuracy: 0.98 Test accuracy: 0.9769\n",
      "31 Train accuracy: 0.99333334 Test accuracy: 0.975\n",
      "32 Train accuracy: 1.0 Test accuracy: 0.9763\n",
      "33 Train accuracy: 0.99333334 Test accuracy: 0.9797\n",
      "34 Train accuracy: 0.99333334 Test accuracy: 0.9768\n",
      "35 Train accuracy: 1.0 Test accuracy: 0.9756\n",
      "36 Train accuracy: 0.99333334 Test accuracy: 0.9768\n",
      "37 Train accuracy: 0.99333334 Test accuracy: 0.9803\n",
      "38 Train accuracy: 0.9866667 Test accuracy: 0.9787\n",
      "39 Train accuracy: 0.99333334 Test accuracy: 0.9686\n",
      "40 Train accuracy: 0.99333334 Test accuracy: 0.9758\n",
      "41 Train accuracy: 0.99333334 Test accuracy: 0.9797\n",
      "42 Train accuracy: 0.99333334 Test accuracy: 0.9736\n",
      "43 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "44 Train accuracy: 0.99333334 Test accuracy: 0.9823\n",
      "45 Train accuracy: 0.99333334 Test accuracy: 0.9779\n",
      "46 Train accuracy: 0.9866667 Test accuracy: 0.9742\n",
      "47 Train accuracy: 1.0 Test accuracy: 0.9783\n",
      "48 Train accuracy: 0.9866667 Test accuracy: 0.9819\n",
      "49 Train accuracy: 0.9866667 Test accuracy: 0.9759\n",
      "50 Train accuracy: 1.0 Test accuracy: 0.9806\n",
      "51 Train accuracy: 0.99333334 Test accuracy: 0.981\n",
      "52 Train accuracy: 1.0 Test accuracy: 0.9791\n",
      "53 Train accuracy: 1.0 Test accuracy: 0.9778\n",
      "54 Train accuracy: 0.99333334 Test accuracy: 0.9747\n",
      "55 Train accuracy: 0.99333334 Test accuracy: 0.9806\n",
      "56 Train accuracy: 0.99333334 Test accuracy: 0.981\n",
      "57 Train accuracy: 0.9866667 Test accuracy: 0.9796\n",
      "58 Train accuracy: 0.98 Test accuracy: 0.9779\n",
      "59 Train accuracy: 1.0 Test accuracy: 0.9812\n",
      "60 Train accuracy: 1.0 Test accuracy: 0.9775\n",
      "61 Train accuracy: 0.99333334 Test accuracy: 0.9774\n",
      "62 Train accuracy: 1.0 Test accuracy: 0.9786\n",
      "63 Train accuracy: 0.98 Test accuracy: 0.9815\n",
      "64 Train accuracy: 1.0 Test accuracy: 0.9791\n",
      "65 Train accuracy: 0.9866667 Test accuracy: 0.9773\n",
      "66 Train accuracy: 1.0 Test accuracy: 0.9823\n",
      "67 Train accuracy: 0.99333334 Test accuracy: 0.9814\n",
      "68 Train accuracy: 1.0 Test accuracy: 0.977\n",
      "69 Train accuracy: 0.99333334 Test accuracy: 0.9724\n",
      "70 Train accuracy: 0.9866667 Test accuracy: 0.9802\n",
      "71 Train accuracy: 0.9866667 Test accuracy: 0.9778\n",
      "72 Train accuracy: 1.0 Test accuracy: 0.983\n",
      "73 Train accuracy: 1.0 Test accuracy: 0.9695\n",
      "74 Train accuracy: 1.0 Test accuracy: 0.9771\n",
      "75 Train accuracy: 0.9866667 Test accuracy: 0.9745\n",
      "76 Train accuracy: 0.99333334 Test accuracy: 0.9776\n",
      "77 Train accuracy: 1.0 Test accuracy: 0.9756\n",
      "78 Train accuracy: 0.9866667 Test accuracy: 0.974\n",
      "79 Train accuracy: 0.99333334 Test accuracy: 0.9795\n",
      "80 Train accuracy: 1.0 Test accuracy: 0.9793\n",
      "81 Train accuracy: 0.98 Test accuracy: 0.9761\n",
      "82 Train accuracy: 0.99333334 Test accuracy: 0.9814\n",
      "83 Train accuracy: 1.0 Test accuracy: 0.978\n",
      "84 Train accuracy: 1.0 Test accuracy: 0.9741\n",
      "85 Train accuracy: 1.0 Test accuracy: 0.9793\n",
      "86 Train accuracy: 1.0 Test accuracy: 0.9826\n",
      "87 Train accuracy: 0.9866667 Test accuracy: 0.9792\n",
      "88 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "89 Train accuracy: 1.0 Test accuracy: 0.9792\n",
      "90 Train accuracy: 1.0 Test accuracy: 0.9805\n",
      "91 Train accuracy: 0.99333334 Test accuracy: 0.9811\n",
      "92 Train accuracy: 0.9866667 Test accuracy: 0.9745\n",
      "93 Train accuracy: 0.99333334 Test accuracy: 0.9799\n",
      "94 Train accuracy: 0.99333334 Test accuracy: 0.9758\n",
      "95 Train accuracy: 0.9866667 Test accuracy: 0.9794\n",
      "96 Train accuracy: 1.0 Test accuracy: 0.9807\n",
      "97 Train accuracy: 0.92 Test accuracy: 0.9163\n",
      "98 Train accuracy: 0.98 Test accuracy: 0.9767\n",
      "99 Train accuracy: 1.0 Test accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "n_steps = 28\n",
    "n_inputs = 28\n",
    "n_neurons = 150\n",
    "n_outputs = 10\n",
    "learning_rate = 0.001\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs]) # 150 x 28 x 28\n",
    "y = tf.placeholder(tf.int32, [None]) #150\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons) # 150 x 28x 150\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                         logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "X_test = mnist.test.images.reshape((-1, n_steps, n_inputs))\n",
    "y_test = mnist.test.labels\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 150\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            X_batch = X_batch.reshape((-1, n_steps, n_inputs))\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print(epoch, \"Train accuracy:\", acc_train,\n",
    "              \"Test accuracy:\", acc_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0928 12:18:37.235973   800 deprecation.py:323] From C:\\Users\\KITCOOP\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0928 12:18:37.508226   800 deprecation.py:323] From <ipython-input-7-d90edfe63300>:35: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W0928 12:18:37.509250   800 deprecation.py:323] From <ipython-input-7-d90edfe63300>:39: static_bidirectional_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell, unroll=True))`, which is equivalent to this API\n",
      "W0928 12:18:38.388904   800 deprecation.py:323] From <ipython-input-7-d90edfe63300>:46: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1280, Minibatch Loss= 1.957317, Training Accuracy= 0.37500\n",
      "Iter 2560, Minibatch Loss= 1.518239, Training Accuracy= 0.46875\n",
      "Iter 3840, Minibatch Loss= 1.348223, Training Accuracy= 0.57031\n",
      "Iter 5120, Minibatch Loss= 1.030315, Training Accuracy= 0.64062\n",
      "Iter 6400, Minibatch Loss= 0.693228, Training Accuracy= 0.77344\n",
      "Iter 7680, Minibatch Loss= 0.672598, Training Accuracy= 0.81250\n",
      "Iter 8960, Minibatch Loss= 0.705023, Training Accuracy= 0.80469\n",
      "Iter 10240, Minibatch Loss= 0.627150, Training Accuracy= 0.81250\n",
      "Iter 11520, Minibatch Loss= 0.647041, Training Accuracy= 0.77344\n",
      "Iter 12800, Minibatch Loss= 0.455562, Training Accuracy= 0.83594\n",
      "Iter 14080, Minibatch Loss= 0.536967, Training Accuracy= 0.82812\n",
      "Iter 15360, Minibatch Loss= 0.364502, Training Accuracy= 0.84375\n",
      "Iter 16640, Minibatch Loss= 0.525852, Training Accuracy= 0.85938\n",
      "Iter 17920, Minibatch Loss= 0.271825, Training Accuracy= 0.88281\n",
      "Iter 19200, Minibatch Loss= 0.305244, Training Accuracy= 0.92188\n",
      "Iter 20480, Minibatch Loss= 0.267676, Training Accuracy= 0.90625\n",
      "Iter 21760, Minibatch Loss= 0.377019, Training Accuracy= 0.91406\n",
      "Iter 23040, Minibatch Loss= 0.443690, Training Accuracy= 0.85938\n",
      "Iter 24320, Minibatch Loss= 0.311176, Training Accuracy= 0.91406\n",
      "Iter 25600, Minibatch Loss= 0.265523, Training Accuracy= 0.89062\n",
      "Iter 26880, Minibatch Loss= 0.397399, Training Accuracy= 0.90625\n",
      "Iter 28160, Minibatch Loss= 0.267343, Training Accuracy= 0.92188\n",
      "Iter 29440, Minibatch Loss= 0.225262, Training Accuracy= 0.94531\n",
      "Iter 30720, Minibatch Loss= 0.144830, Training Accuracy= 0.94531\n",
      "Iter 32000, Minibatch Loss= 0.299194, Training Accuracy= 0.89844\n",
      "Iter 33280, Minibatch Loss= 0.083682, Training Accuracy= 0.97656\n",
      "Iter 34560, Minibatch Loss= 0.218851, Training Accuracy= 0.93750\n",
      "Iter 35840, Minibatch Loss= 0.236995, Training Accuracy= 0.90625\n",
      "Iter 37120, Minibatch Loss= 0.163881, Training Accuracy= 0.94531\n",
      "Iter 38400, Minibatch Loss= 0.271264, Training Accuracy= 0.92188\n",
      "Iter 39680, Minibatch Loss= 0.237823, Training Accuracy= 0.91406\n",
      "Iter 40960, Minibatch Loss= 0.187032, Training Accuracy= 0.92969\n",
      "Iter 42240, Minibatch Loss= 0.194283, Training Accuracy= 0.93750\n",
      "Iter 43520, Minibatch Loss= 0.120311, Training Accuracy= 0.96875\n",
      "Iter 44800, Minibatch Loss= 0.241423, Training Accuracy= 0.95312\n",
      "Iter 46080, Minibatch Loss= 0.169167, Training Accuracy= 0.94531\n",
      "Iter 47360, Minibatch Loss= 0.251879, Training Accuracy= 0.94531\n",
      "Iter 48640, Minibatch Loss= 0.166036, Training Accuracy= 0.96094\n",
      "Iter 49920, Minibatch Loss= 0.158630, Training Accuracy= 0.92188\n",
      "Iter 51200, Minibatch Loss= 0.165004, Training Accuracy= 0.95312\n",
      "Iter 52480, Minibatch Loss= 0.168194, Training Accuracy= 0.96094\n",
      "Iter 53760, Minibatch Loss= 0.152407, Training Accuracy= 0.96094\n",
      "Iter 55040, Minibatch Loss= 0.197773, Training Accuracy= 0.93750\n",
      "Iter 56320, Minibatch Loss= 0.113004, Training Accuracy= 0.96875\n",
      "Iter 57600, Minibatch Loss= 0.076235, Training Accuracy= 0.97656\n",
      "Iter 58880, Minibatch Loss= 0.099278, Training Accuracy= 0.97656\n",
      "Iter 60160, Minibatch Loss= 0.075827, Training Accuracy= 0.96875\n",
      "Iter 61440, Minibatch Loss= 0.104715, Training Accuracy= 0.96094\n",
      "Iter 62720, Minibatch Loss= 0.116597, Training Accuracy= 0.98438\n",
      "Iter 64000, Minibatch Loss= 0.100324, Training Accuracy= 0.96875\n",
      "Iter 65280, Minibatch Loss= 0.114771, Training Accuracy= 0.97656\n",
      "Iter 66560, Minibatch Loss= 0.083958, Training Accuracy= 0.98438\n",
      "Iter 67840, Minibatch Loss= 0.090373, Training Accuracy= 0.97656\n",
      "Iter 69120, Minibatch Loss= 0.080239, Training Accuracy= 0.98438\n",
      "Iter 70400, Minibatch Loss= 0.171316, Training Accuracy= 0.96094\n",
      "Iter 71680, Minibatch Loss= 0.084594, Training Accuracy= 0.96094\n",
      "Iter 72960, Minibatch Loss= 0.111686, Training Accuracy= 0.97656\n",
      "Iter 74240, Minibatch Loss= 0.074778, Training Accuracy= 0.97656\n",
      "Iter 75520, Minibatch Loss= 0.133952, Training Accuracy= 0.97656\n",
      "Iter 76800, Minibatch Loss= 0.036491, Training Accuracy= 0.99219\n",
      "Iter 78080, Minibatch Loss= 0.086287, Training Accuracy= 0.96094\n",
      "Iter 79360, Minibatch Loss= 0.097430, Training Accuracy= 0.97656\n",
      "Iter 80640, Minibatch Loss= 0.207620, Training Accuracy= 0.95312\n",
      "Iter 81920, Minibatch Loss= 0.079424, Training Accuracy= 0.97656\n",
      "Iter 83200, Minibatch Loss= 0.080341, Training Accuracy= 0.97656\n",
      "Iter 84480, Minibatch Loss= 0.103461, Training Accuracy= 0.97656\n",
      "Iter 85760, Minibatch Loss= 0.084681, Training Accuracy= 0.98438\n",
      "Iter 87040, Minibatch Loss= 0.140362, Training Accuracy= 0.94531\n",
      "Iter 88320, Minibatch Loss= 0.227128, Training Accuracy= 0.94531\n",
      "Iter 89600, Minibatch Loss= 0.185787, Training Accuracy= 0.94531\n",
      "Iter 90880, Minibatch Loss= 0.099228, Training Accuracy= 0.95312\n",
      "Iter 92160, Minibatch Loss= 0.068883, Training Accuracy= 0.97656\n",
      "Iter 93440, Minibatch Loss= 0.128188, Training Accuracy= 0.96875\n",
      "Iter 94720, Minibatch Loss= 0.091001, Training Accuracy= 0.96875\n",
      "Iter 96000, Minibatch Loss= 0.132586, Training Accuracy= 0.97656\n",
      "Iter 97280, Minibatch Loss= 0.088821, Training Accuracy= 0.97656\n",
      "Iter 98560, Minibatch Loss= 0.070457, Training Accuracy= 0.98438\n",
      "Iter 99840, Minibatch Loss= 0.074396, Training Accuracy= 0.97656\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.9765625\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "n_input = 28\n",
    "n_steps = 28\n",
    "n_hidden = 128\n",
    "n_classes = 10\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([2*n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "   \n",
    "def BiRNN(x, weights, biases):\n",
    "    x = tf.transpose(x, [1, 0, 2])\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "    x = tf.split(axis=0, num_or_size_splits=n_steps, value=x)\n",
    "    lstm_fw_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    lstm_bw_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    try:\n",
    "        outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
    "                                              dtype=tf.float32)\n",
    "    except Exception:\n",
    "        outputs = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
    "                                        dtype=tf.float32)\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "pred = BiRNN(x, weights, biases)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        if step % display_step == 0:\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "sess.run(accuracy, feed_dict={x: test_data, y: test_label})) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 4)\n",
      "array([[[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]]], dtype=float32)\n",
      "array([[[ 0.45403883, -0.35677183],\n",
      "        [ 0.41196236, -0.32082513],\n",
      "        [-0.4225641 , -0.5356234 ],\n",
      "        [ 0.34799594, -0.41626254]]], dtype=float32)\n",
      "(1, 4, 2)\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "ops.reset_default_graph()\n",
    "pp=pprint.PrettyPrinter(indent=4)\n",
    "sess=tf.InteractiveSession()\n",
    "h=[1,0,0,0]\n",
    "e=[0,1,0,0]\n",
    "i=[0,0,1,0]\n",
    "o=[0,0,0,1]\n",
    "with tf.variable_scope('five_sequences') as scope :\n",
    "    hidden_size=2\n",
    "    cell=tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "    x_data=np.array([[h,e,i,o]],dtype=np.float32)\n",
    "    print(x_data.shape)\n",
    "    pp.pprint(x_data)\n",
    "    outputs,states=tf.nn.dynamic_rnn(cell,x_data,dtype=tf.float32)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())\n",
    "    print(outputs.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0928 13:05:01.865700   800 deprecation.py:323] From <ipython-input-12-a3c183a81079>:44: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 3.561081\n",
      "Epoch: 0002 cost = 2.668203\n",
      "Epoch: 0003 cost = 1.853582\n",
      "Epoch: 0004 cost = 1.132326\n",
      "Epoch: 0005 cost = 0.983332\n",
      "Epoch: 0006 cost = 0.637660\n",
      "Epoch: 0007 cost = 0.597358\n",
      "Epoch: 0008 cost = 0.563899\n",
      "Epoch: 0009 cost = 0.481921\n",
      "Epoch: 0010 cost = 0.236148\n",
      "Epoch: 0011 cost = 0.300621\n",
      "Epoch: 0012 cost = 0.377479\n",
      "Epoch: 0013 cost = 0.200666\n",
      "Epoch: 0014 cost = 0.184826\n",
      "Epoch: 0015 cost = 0.279425\n",
      "Epoch: 0016 cost = 0.121217\n",
      "Epoch: 0017 cost = 0.117530\n",
      "Epoch: 0018 cost = 0.037932\n",
      "Epoch: 0019 cost = 0.058749\n",
      "Epoch: 0020 cost = 0.089728\n",
      "Epoch: 0021 cost = 0.028341\n",
      "Epoch: 0022 cost = 0.071038\n",
      "Epoch: 0023 cost = 0.087162\n",
      "Epoch: 0024 cost = 0.089899\n",
      "Epoch: 0025 cost = 0.049051\n",
      "Epoch: 0026 cost = 0.066015\n",
      "Epoch: 0027 cost = 0.060427\n",
      "Epoch: 0028 cost = 0.075622\n",
      "Epoch: 0029 cost = 0.007315\n",
      "Epoch: 0030 cost = 0.008334\n",
      "\n",
      "=== 예측 결과 ===\n",
      "입력값: ['wor ', 'woo ', 'dee ', 'div ', 'col ', 'coo ', 'loa ', 'lov ', 'kis ', 'kin ']\n",
      "예측값: ['word', 'wood', 'deep', 'dive', 'cold', 'cool', 'load', 'love', 'kiss', 'kind']\n",
      "정확도: 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# 사전 => 숫자화\n",
    "char_arr = ['a', 'b', 'c', 'd', 'e', 'f', 'g',\n",
    "            'h', 'i', 'j', 'k', 'l', 'm', 'n',\n",
    "            'o', 'p', 'q', 'r', 's', 't', 'u',\n",
    "            'v', 'w', 'x', 'y', 'z']\n",
    "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "dic_len = len(num_dic)\n",
    "seq_data = ['word', 'wood', 'deep', 'dive', 'cold', 'cool', 'load', 'love', 'kiss', 'kind']\n",
    "\n",
    "\n",
    "def make_batch(seq_data):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "    for seq in seq_data:\n",
    "        input = [num_dic[n] for n in seq[:-1]] \n",
    "        target = num_dic[seq[-1]]\n",
    "  \n",
    "        input_batch.append(np.eye(dic_len)[input])\n",
    "        target_batch.append(target)\n",
    "    return input_batch, target_batch\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_hidden = 128 \n",
    "total_epoch = 30\n",
    "n_step = 3\n",
    "n_input = n_class = dic_len\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_step, n_input])\n",
    "Y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "\n",
    "W = tf.Variable(tf.random_normal([n_hidden,n_class]))\n",
    "b = tf.Variable(tf.random_normal([n_class]))\n",
    "\n",
    "\n",
    "cell1 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "cell1 = tf.nn.rnn_cell.DropoutWrapper(cell1, output_keep_prob=0.5, seed=10)\n",
    "cell2 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "multi_cell = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2])\n",
    "outputs, states = tf.nn.dynamic_rnn(multi_cell, X, dtype=tf.float32)\n",
    "\n",
    "\n",
    "outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "outputs = outputs[-1]\n",
    "model = tf.matmul(outputs, W) + b \n",
    "cost = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=model, labels=Y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "input_batch, target_batch = make_batch(seq_data)\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    _, loss = sess.run([optimizer, cost],\n",
    "                       feed_dict={X: input_batch, Y: target_batch})\n",
    "    print('Epoch:', '%04d' % (epoch + 1),\n",
    "          'cost =', '{:.6f}'.format(loss))\n",
    "   \n",
    "\n",
    "prediction = tf.cast(tf.argmax(model, 1), tf.int32)\n",
    "prediction_check = tf.equal(prediction, Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction_check, tf.float32))\n",
    "\n",
    "input_batch, target_batch = make_batch(seq_data)\n",
    "\n",
    "predict, accuracy_val = sess.run([prediction, accuracy],\n",
    "                                 feed_dict={X: input_batch, Y: target_batch})\n",
    "\n",
    "\n",
    "predict_words = []\n",
    "for idx, val in enumerate(seq_data):\n",
    "    last_char = char_arr[predict[idx]]\n",
    "    predict_words.append(val[:3] + last_char)\n",
    "\n",
    "print('\\n=== 예측 결과 ===')\n",
    "print('입력값:', [w[:3] + ' ' for w in seq_data])\n",
    "print('예측값:', predict_words)\n",
    "print('정확도:', accuracy_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "Epoch: 0001 cost = 3.665734\n",
      "Epoch: 0002 cost = 2.347632\n",
      "Epoch: 0003 cost = 1.568537\n",
      "Epoch: 0004 cost = 0.990969\n",
      "Epoch: 0005 cost = 0.433573\n",
      "Epoch: 0006 cost = 0.503855\n",
      "Epoch: 0007 cost = 0.362320\n",
      "Epoch: 0008 cost = 0.192028\n",
      "Epoch: 0009 cost = 0.325697\n",
      "Epoch: 0010 cost = 0.135989\n",
      "Epoch: 0011 cost = 0.263769\n",
      "Epoch: 0012 cost = 0.066751\n",
      "Epoch: 0013 cost = 0.197711\n",
      "Epoch: 0014 cost = 0.249975\n",
      "Epoch: 0015 cost = 0.114573\n",
      "Epoch: 0016 cost = 0.240322\n",
      "Epoch: 0017 cost = 0.158376\n",
      "Epoch: 0018 cost = 0.055535\n",
      "Epoch: 0019 cost = 0.120976\n",
      "Epoch: 0020 cost = 0.151166\n",
      "Epoch: 0021 cost = 0.039259\n",
      "Epoch: 0022 cost = 0.059280\n",
      "Epoch: 0023 cost = 0.018121\n",
      "Epoch: 0024 cost = 0.016082\n",
      "Epoch: 0025 cost = 0.015466\n",
      "Epoch: 0026 cost = 0.022617\n",
      "Epoch: 0027 cost = 0.012235\n",
      "Epoch: 0028 cost = 0.013053\n",
      "Epoch: 0029 cost = 0.005268\n",
      "Epoch: 0030 cost = 0.007888\n",
      "Epoch: 0031 cost = 0.008089\n",
      "Epoch: 0032 cost = 0.011316\n",
      "Epoch: 0033 cost = 0.002053\n",
      "Epoch: 0034 cost = 0.001637\n",
      "Epoch: 0035 cost = 0.003552\n",
      "Epoch: 0036 cost = 0.000968\n",
      "Epoch: 0037 cost = 0.003103\n",
      "Epoch: 0038 cost = 0.001898\n",
      "Epoch: 0039 cost = 0.003188\n",
      "Epoch: 0040 cost = 0.001353\n",
      "Epoch: 0041 cost = 0.001909\n",
      "Epoch: 0042 cost = 0.005934\n",
      "Epoch: 0043 cost = 0.002810\n",
      "Epoch: 0044 cost = 0.001249\n",
      "Epoch: 0045 cost = 0.004229\n",
      "Epoch: 0046 cost = 0.001715\n",
      "Epoch: 0047 cost = 0.001970\n",
      "Epoch: 0048 cost = 0.004118\n",
      "Epoch: 0049 cost = 0.001690\n",
      "Epoch: 0050 cost = 0.003747\n",
      "Epoch: 0051 cost = 0.001514\n",
      "Epoch: 0052 cost = 0.003574\n",
      "Epoch: 0053 cost = 0.000981\n",
      "Epoch: 0054 cost = 0.001897\n",
      "Epoch: 0055 cost = 0.000672\n",
      "Epoch: 0056 cost = 0.000457\n",
      "Epoch: 0057 cost = 0.003203\n",
      "Epoch: 0058 cost = 0.001944\n",
      "Epoch: 0059 cost = 0.000690\n",
      "Epoch: 0060 cost = 0.000569\n",
      "Epoch: 0061 cost = 0.000911\n",
      "Epoch: 0062 cost = 0.001073\n",
      "Epoch: 0063 cost = 0.000729\n",
      "Epoch: 0064 cost = 0.000938\n",
      "Epoch: 0065 cost = 0.000888\n",
      "Epoch: 0066 cost = 0.000625\n",
      "Epoch: 0067 cost = 0.000734\n",
      "Epoch: 0068 cost = 0.000236\n",
      "Epoch: 0069 cost = 0.000484\n",
      "Epoch: 0070 cost = 0.001072\n",
      "Epoch: 0071 cost = 0.000881\n",
      "Epoch: 0072 cost = 0.000542\n",
      "Epoch: 0073 cost = 0.002716\n",
      "Epoch: 0074 cost = 0.000416\n",
      "Epoch: 0075 cost = 0.000483\n",
      "Epoch: 0076 cost = 0.000810\n",
      "Epoch: 0077 cost = 0.000881\n",
      "Epoch: 0078 cost = 0.000472\n",
      "Epoch: 0079 cost = 0.000331\n",
      "Epoch: 0080 cost = 0.000594\n",
      "Epoch: 0081 cost = 0.002442\n",
      "Epoch: 0082 cost = 0.000344\n",
      "Epoch: 0083 cost = 0.000721\n",
      "Epoch: 0084 cost = 0.000551\n",
      "Epoch: 0085 cost = 0.000335\n",
      "Epoch: 0086 cost = 0.000454\n",
      "Epoch: 0087 cost = 0.000520\n",
      "Epoch: 0088 cost = 0.000171\n",
      "Epoch: 0089 cost = 0.001273\n",
      "Epoch: 0090 cost = 0.000606\n",
      "Epoch: 0091 cost = 0.001523\n",
      "Epoch: 0092 cost = 0.001028\n",
      "Epoch: 0093 cost = 0.000347\n",
      "Epoch: 0094 cost = 0.000765\n",
      "Epoch: 0095 cost = 0.000245\n",
      "Epoch: 0096 cost = 0.000130\n",
      "Epoch: 0097 cost = 0.000642\n",
      "Epoch: 0098 cost = 0.001045\n",
      "Epoch: 0099 cost = 0.000858\n",
      "Epoch: 0100 cost = 0.000165\n",
      "word -> 단어\n",
      "love -> 사랑\n",
      "abcd -> 소어\n",
      "wodr -> 나무\n",
      "loev -> 사랑\n",
      "abcd -> 사어\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz단어나무놀이소녀키스사랑']\n",
    "\n",
    "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "dic_len = len(num_dic)\n",
    "print(dic_len)\n",
    "seq_data = [['word', '단어'], ['wood', '나무'],\n",
    "            ['game', '놀이'], ['girl', '소녀'],\n",
    "            ['kiss', '키스'], ['love', '사랑']]\n",
    "\n",
    " \n",
    "\n",
    "def make_batch(seq_data):\n",
    "    input_batch = []\n",
    "    output_batch = []\n",
    "    target_batch = []\n",
    "    for seq in seq_data:\n",
    "        input = [num_dic[n] for n in seq[0]]\n",
    "        output = [num_dic[n] for n in ('S' + seq[1])]\n",
    "        target = [num_dic[n] for n in (seq[1] + 'E')]\n",
    "        input_batch.append(np.eye(dic_len)[input])\n",
    "        output_batch.append(np.eye(dic_len)[output])\n",
    "        target_batch.append(target)\n",
    "    return input_batch, output_batch, target_batch\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_hidden = 128\n",
    "total_epoch = 100\n",
    "n_class = n_input = dic_len\n",
    "\n",
    "enc_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
    "dec_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
    "targets = tf.placeholder(tf.int64, [None, None])\n",
    "\n",
    " \n",
    "with tf.variable_scope('encode'):\n",
    "    enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5,seed=100)\n",
    "    outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input,  dtype=tf.float32)\n",
    "with tf.variable_scope('decode'):\n",
    "    dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
    "    outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input,\n",
    "                                            initial_state=enc_states,\n",
    "                                            dtype=tf.float32)\n",
    "   \n",
    "\n",
    "model = tf.layers.dense(outputs, n_class, activation=None)\n",
    "\n",
    "cost = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=model, labels=targets))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "input_batch, output_batch, target_batch = make_batch(seq_data)\n",
    "for epoch in range(total_epoch):\n",
    "    _, loss = sess.run([optimizer, cost],\n",
    "                       feed_dict={enc_input: input_batch,\n",
    "                                  dec_input: output_batch,\n",
    "                                  targets: target_batch})\n",
    "    print('Epoch:', '%04d' % (epoch + 1),\n",
    "          'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "def translate(word):\n",
    "    seq_data = [word, 'P' * len(word)]\n",
    "    input_batch, output_batch, target_batch = make_batch([seq_data])\n",
    "    prediction = tf.argmax(model, 2)\n",
    "\n",
    "    result = sess.run(prediction,\n",
    "                      feed_dict={enc_input: input_batch,\n",
    "                                 dec_input: output_batch,\n",
    "                                 targets: target_batch})\n",
    "    decoded = [char_arr[i] for i in result[0]]\n",
    "\n",
    "    end = decoded.index('E')\n",
    "    translated = ''.join(decoded[:end])\n",
    "\n",
    "    return translated\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "print('word ->', translate('word'))\n",
    "print('love ->', translate('love'))\n",
    "print('abcd ->', translate('abcd'))\n",
    "\n",
    "print('wodr ->', translate('wodr'))\n",
    "print('loev ->', translate('loev'))\n",
    "print('abcd ->', translate('abcd')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow 2.0\n",
    "# -tf.keras 가 core\n",
    "# -텐서플로우 서빙(클라우드 서비스) : 모델을 실시간 서비스하는 서버\n",
    "\n",
    "\"\"\"\n",
    "Anaconda prompt>\n",
    "pip install pyyaml h5py\n",
    "pip install numpy scipy\n",
    "pip install pillow\n",
    "conda install pydot\n",
    "conda install pydotplus\n",
    "conda install keras\n",
    "\n",
    "https://graphviz.gitlab.io/_pages/Download/Download_windows.html\n",
    "에서 Graphviz windows version msi파일 다운로드 및 설치, 환경변수 PATH 설정\n",
    "(C:\\Program Files (x86)\\Graphviz2.38\\bin) 추가\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0928 16:05:02.076240  8704 deprecation_wrapper.py:119] From C:\\Users\\KITCOOP\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0928 16:05:02.089379  8704 deprecation_wrapper.py:119] From C:\\Users\\KITCOOP\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0928 16:05:02.092371  8704 deprecation_wrapper.py:119] From C:\\Users\\KITCOOP\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "W0928 16:05:02.105337  8704 deprecation_wrapper.py:119] From C:\\Users\\KITCOOP\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0928 16:05:02.214101  8704 deprecation_wrapper.py:119] From C:\\Users\\KITCOOP\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0928 16:05:02.218113  8704 deprecation_wrapper.py:119] From C:\\Users\\KITCOOP\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential #케라스 모델 생성\n",
    "from keras.layers import Dense\n",
    "#모델정의 -> compile(tensorflow, theano, CNTK 레퍼) -> fitting -> prediction\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "\n",
    "base_series = [0,1,2,3,4,5,6,7,8,9]\n",
    "series = base_series*10\n",
    "seq_length = len(base_series)\n",
    "X = []\n",
    "Y = []\n",
    "def unit(index): return [1.0 if i == index else 0.0 for i in range(seq_length)]\n",
    "\n",
    "for i in range(0, len(series) - seq_length, 1):\n",
    "    X.append(series[i:i + seq_length])\n",
    "    Y.append(unit(np.mod(i, seq_length)))\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(seq_length, input_dim=X.shape[1], kernel_initializer='normal', activation='softmax')) \n",
    "\n",
    "model.compile(loss='mean_absolute_error', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.fit(X, Y, epochs=350, verbose=0)\n",
    "scores = model.evaluate(X, Y, verbose=0)\n",
    "print(\"정확도: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: -114.53 (82.56) MSE\n",
      "Standardized: -29.36 (27.57) MSE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([29.819601, 24.979347, 34.326424, 32.257004, 31.885862],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor #케라스와 싸이킷 연결\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold # 자동으로 데이터를 분할해서 다양한 경우의 수로 모델을 훈련\n",
    "from sklearn.preprocessing import StandardScaler # 전처리기능 : z점수 정규화\n",
    "from sklearn.pipeline import Pipeline\n",
    "# 데이터를 분할할 때, 전처리를 하고 나누는 것과 나누고 전처리하는 것 중 뭐가 맞을까?\n",
    "# 직관적인 판단과 달리 분할하고 나서 전처리 하는게 맞음! (두 데이터의 분포가 다르기 때문, ex: standard scaling 할 때 )\n",
    "\n",
    "dataframe = pandas.read_csv(\"housing.csv\", delim_whitespace=True, header=None)\n",
    "dataset = dataframe.values\n",
    "X = dataset[:,0:13]\n",
    "Y = dataset[:,13]\n",
    "\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=100, batch_size=5, verbose=0)\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
    "\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=baseline_model, epochs=50, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
    "\n",
    "pipeline.fit(X,Y)\n",
    "pipeline.predict(X[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KITCOOP\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\KITCOOP\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "W0928 16:54:57.050263  8704 deprecation.py:323] From C:\\Users\\KITCOOP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.679688 using {'learn_rate': 0.001, 'momentum': 0.4}\n",
      "0.555990 (0.153137) with: {'learn_rate': 0.001, 'momentum': 0.0}\n",
      "0.662760 (0.032578) with: {'learn_rate': 0.001, 'momentum': 0.2}\n",
      "0.679688 (0.005524) with: {'learn_rate': 0.001, 'momentum': 0.4}\n",
      "0.661458 (0.035132) with: {'learn_rate': 0.001, 'momentum': 0.6}\n",
      "0.662760 (0.009744) with: {'learn_rate': 0.001, 'momentum': 0.8}\n",
      "0.645833 (0.023073) with: {'learn_rate': 0.001, 'momentum': 0.9}\n",
      "0.664063 (0.024080) with: {'learn_rate': 0.01, 'momentum': 0.0}\n",
      "0.455729 (0.146518) with: {'learn_rate': 0.01, 'momentum': 0.2}\n",
      "0.455729 (0.146518) with: {'learn_rate': 0.01, 'momentum': 0.4}\n",
      "0.572917 (0.134575) with: {'learn_rate': 0.01, 'momentum': 0.6}\n",
      "0.572917 (0.134575) with: {'learn_rate': 0.01, 'momentum': 0.8}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.01, 'momentum': 0.9}\n",
      "0.427083 (0.134575) with: {'learn_rate': 0.1, 'momentum': 0.0}\n",
      "0.466146 (0.149269) with: {'learn_rate': 0.1, 'momentum': 0.2}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.1, 'momentum': 0.4}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.1, 'momentum': 0.6}\n",
      "0.533854 (0.149269) with: {'learn_rate': 0.1, 'momentum': 0.8}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.1, 'momentum': 0.9}\n",
      "0.427083 (0.134575) with: {'learn_rate': 0.2, 'momentum': 0.0}\n",
      "0.455729 (0.146518) with: {'learn_rate': 0.2, 'momentum': 0.2}\n",
      "0.348958 (0.024774) with: {'learn_rate': 0.2, 'momentum': 0.4}\n",
      "0.348958 (0.024774) with: {'learn_rate': 0.2, 'momentum': 0.6}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.2, 'momentum': 0.8}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.2, 'momentum': 0.9}\n",
      "0.533854 (0.149269) with: {'learn_rate': 0.3, 'momentum': 0.0}\n",
      "0.572917 (0.134575) with: {'learn_rate': 0.3, 'momentum': 0.2}\n",
      "0.455729 (0.146518) with: {'learn_rate': 0.3, 'momentum': 0.4}\n",
      "0.544271 (0.146518) with: {'learn_rate': 0.3, 'momentum': 0.6}\n",
      "0.572917 (0.134575) with: {'learn_rate': 0.3, 'momentum': 0.8}\n",
      "0.572917 (0.134575) with: {'learn_rate': 0.3, 'momentum': 0.9}\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "def create_model(learn_rate=0.01, momentum=0):\n",
    "\n",
    " model = Sequential()\n",
    " model.add(Dense(12, input_dim=8, activation='relu'))\n",
    " model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    " optimizer = SGD(lr=learn_rate, momentum=momentum)\n",
    " model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    " return model\n",
    "\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "dataset = numpy.loadtxt(\"pima.csv\", delimiter=\",\")\n",
    "\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "param_grid = dict(learn_rate=learn_rate, momentum=momentum)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X, Y)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
